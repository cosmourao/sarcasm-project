\thispagestyle{plain}

\section{Conclusion}

While this project had its limitations, it was possible to see that indeed transformer networks such as distilBERT are more suitable for highly context-dependent NLP tasks such as sarcasm detection. The power of transfomers comes with a caveat: these models are significantly more computationally demanding and need to use parallelization to be efficient. The results could be improved via a more optimized dataset and fine-tuning.

In conclusion, a not-so-black-box view of this project allowed for an initial contact with the transformer architecture, its power, and its importance to the NLP world.