@inproceedings{gonzalez2011,
  title={Identifying sarcasm in Twitter: a closer look},
  author={Gonz{\'a}lez-Ib{\'a}nez, Roberto and Muresan, Smaranda and Wacholder, Nina},
  booktitle={Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
  pages={581--586},
  year={2011}
}

@inproceedings{patro2019,
	title={A deep-learning framework to detect sarcasm targets},
	author={Patro, Jasabanta and Bansal, Srijan and Mukherjee, Animesh},
	booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
	pages={6337--6343},
	year={2019}
}

@article{misra2019sarcasm,
	title={Sarcasm Detection using Hybrid Neural Network},
	author={Misra, Rishabh and Arora, Prahal},
	journal={arXiv preprint arXiv:1908.07414},
	year={2019}
}

@article{potamias2020,
	title={A transformer-based approach to irony and sarcasm detection},
	volume={32},
	ISSN={1433-3058},
	url={http://dx.doi.org/10.1007/s00521-020-05102-3},
	DOI={10.1007/s00521-020-05102-3},
	number={23},
	journal={Neural Computing and Applications},
	publisher={Springer Science and Business Media LLC},
	author={Potamias, Rolandos Alexandros and Siolas, Georgios and Stafylopatis, Andreas - Georgios},
	year={2020},
	month={Jun},
	pages={17309–17320}
}

@book{manning2008introduction,
	abstract = {Class-tested and coherent, this textbook teaches classical and web information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. It gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective.},
	added-at = {2010-12-20T06:06:48.000+0100},
	address = {Cambridge, UK},
	author = {Manning, Christopher D. and Raghavan, Prabhakar and Schütze, Hinrich},
	biburl = {https://www.bibsonomy.org/bibtex/29f4ab13e07b48b9723113aa74224be65/folke},
	interhash = {2e574e46b7668a7268e7f02b46f4d9bb},
	intrahash = {9f4ab13e07b48b9723113aa74224be65},
	isbn = {978-0-521-86571-5},
	keywords = {book information introduction ir retrieval},
	publisher = {Cambridge University Press},
	timestamp = {2010-12-20T06:06:48.000+0100},
	title = {Introduction to Information Retrieval},
	howpublishsed = {\url{http://nlp.stanford.edu/IR-book/information-retrieval-book.html}},
	year = 2008
}

@unpublished{kuhlmann-lec2-2020,
	author = "M. Kuhlmann",
	year = 2020,
	title = "Text classification",
	note = "732A92/TDDE16 Text Mining lecture slides, Linköping University",
}

@article{mikolov2013efficient,
	title={Efficient Estimation of Word Representations in Vector Space}, 
	author={Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
	year={2013},
	eprint={1301.3781},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@article{ethayarajh2020,
	title={BERT, ELMo, GPT-2: How Contextual are Contextualized Word Representations?},
	author={Kawin Ethayarajh},
	howpublished={\url{http://ai.stanford.edu/blog/contextual/}},
	journal={The Stanford AI Lab Blog},
	publisher={Stanford University},
	year={2020},
	month={Mar}
}

@unpublished{chaubard2016,
	author = {F. Chaubard and Rohit Mundar and Richard Socher},
	year = {2016},
	title = {CS 224D: Deep Learning for NLP},
	note = {Lecture Notes: Part I. Standford University}
}

@misc{devlin2019bert,
	title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
	author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
	year={2019},
	eprint={1810.04805},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@misc{vaswani2017attention,
	title={Attention Is All You Need}, 
	author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
	year={2017},
	eprint={1706.03762},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@misc{sanh2020distilbert,
	title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter}, 
	author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
	year={2020},
	eprint={1910.01108},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@unpublished{devlin2019ppt,
	author = {Jacob Devlin},
	year = {2019},
	title = {Contextual Word Representations with BERT and Other Pre-trained Language Models},
	note = {Powerpoint presentation. Standford University}
}


@unpublished{pena2019,
	author = {José M. Peña},
	year = {2019},
	title = {Lecture 3b Block 1: Support Vector Machines},
	note = {732A99/TDDE01 Machine Learning lecture slides, Linköping University}
}

@misc{hsu2003,
	title={A practical guide to support vector classification},
	author={Hsu, Chih-Wei and Chang, Chih-Chung and Lin, Chih-Jen and others},
	year={2003},
	publisher={Taipei}
}

@article{alammar2019,
	title={A Visual Guide to Using BERT for the First Time},
	author={Jay Alammar},
	howpublished={\url{https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/}},
	journal={Jay Alammar's personal Blog},
	year={2019},
	month={Nov}
}

@misc{halthor2020,
	author = {Ajay Halthor},
	year = {2020},
	title = {BERT Neural Network - EXPLAINED!},
	howpublished = {\url{https://www.youtube.com/watch?v=xI0HHN5XKDo}},
	urldate = {2021-03-10} %date of last access
}

@software{spacy,
	author = {Honnibal, Matthew and Montani, Ines and Van Landeghem, Sofie and Boyd, Adriane},
	title = {{spaCy: Industrial-strength Natural Language Processing in Python}},
	year = 2020,
	publisher = {Zenodo},
	doi = {10.5281/zenodo.1212303},
	url = {https://doi.org/10.5281/zenodo.1212303}
}

@misc{ghodsi20171,
	author = {Ali Ghodsi},
	year = {2017},
	title = {Ali Ghodsi, Lec 12: Neural Networks, Autoencoders, Word2Vec},
	howpublished = {\url{https://youtu.be/syWB-YMYZvI}},
	urldate = {2021-03-10} %date of last access
}

@misc{ghodsi20172,
	author = {Ali Ghodsi},
	year = {2017},
	title = {Ali Ghodsi, Lec 13: Word2Vec Skip-Gram},
	howpublished = {\url{https://youtu.be/GMCwS7tS5ZM}},
	urldate = {2021-03-10} %date of last access
}